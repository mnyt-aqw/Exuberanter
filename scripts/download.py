#!/usr/bin/env python3

'''
A script to download articles contained in `search.json` generated by the
`search.py`.

The first (and only) argument is the email address to use with the Entrez API.

The program currently only downloads open-access articles from PubMed Central
(using FTP). This may be changed in the future to support more sources. The
program output consists of folders, one for each article, containing a PDF and
XML version of the article, figures and additional metadata. The output file
`results.json` contains the paths of all folders relative to CWD.

For all articles the PMC metadata is downloaded and placed within the article
foler.

Note: FTP is used instead of the Entrez API for the main article download, to
get figures and other attachements.
'''

from Bio import Entrez
import xml.etree.ElementTree as ET
import ftplib
import tarfile
import shutil
import sys
import json
import os

# The path of the search results file
SEARCH_FILE = './output/search/results.json'

# The export directory
EXPORT_DIRECTORY = './output/download'

# The size of each download batch (number of articles per Entrez call)
BATCH_SIZE = 50

# Remember to specify a email address to the script for the Entrez API
if len(sys.argv) == 2:
    Entrez.email = sys.argv[1]
else:
    print('The program has to be given a email address for Entrez')
    exit(-1)

# Create a export directory, ignore if already created
os.makedirs(EXPORT_DIRECTORY, exist_ok=True)

# Load the search results from given `search.json` file
if os.path.isfile(SEARCH_FILE) and (file := open(SEARCH_FILE)):
    data = json.load(file)
else:
    print('Failed to load search results, are you sure you have ran the `search.py` script yet?')
    exit(-1)

print('Downloading all article metadata from PubMed ... ', flush=True, end='')
handle = Entrez.efetch(db='pubmed', id=data['pmids'], rettype='docsum', retmode='xml')
xml_data = Entrez.read(handle)
print(f'done ({len(xml_data)} articles)')

# Export all article metadata to per-article directories under `metadata.json`
article_paths = {}
for article in xml_data:
    # Create the article id
    pm_id = article['ArticleIds']['pubmed'][0]
    article_path = f'{EXPORT_DIRECTORY}/{pm_id}'
    os.makedirs(article_path, exist_ok=True)

    if file := open(file_path := f'{article_path}/metadata.json', 'w+'):
        json.dump(article, file)
        article_paths[pm_id] = article_path
    else:
        print(f'Failed to open file {file_path}')
        exit(-1)

# Export all article extras to individual files with the name `abstract.json'
print('Downloading article abstracts from PubMed ... ', flush=True, end='')
handle = Entrez.efetch(db='pubmed', id=data['pmids'], rettype='abstract', retmode='xml')
abstract_data = Entrez.read(handle)

abstract_count = 0
for article in abstract_data['PubmedArticle']:
    article = article['MedlineCitation']
    pm_id = article['PMID']
    article_path = f'{EXPORT_DIRECTORY}/{pm_id}'

    if not 'Abstract' in article['Article']:
        continue

    if file := open(file_path := f'{article_path}/abstract.json', 'w+'):
        json.dump({'text': article['Article']['Abstract']['AbstractText'][0]}, file)
        abstract_count += 1

print(f'done ({abstract_count} articles)')

# Identify all articles from PubMed Central
print(f'Identifying articles in PubMed Central ... ', flush=True, end='')
pmc_ids = {}
for article in xml_data:
    pm_id = article['ArticleIds']['pubmed'][0]
    if 'pmc' in article['ArticleIds']:
        pmc_ids[pm_id] = article['ArticleIds']['pmc']
print(f'done (found {len(pmc_ids)}/{len(xml_data)} articles)')

# Setup FTP connection to NCBI PMC database
ftp = ftplib.FTP('ftp.ncbi.nlm.nih.gov')
ftp.login()
ftp.cwd('/pub/pmc/')

# Download PMC open-access index if needed. It contains all the information
# needed to find each article
print('Fetching PMC open-access file list ... ', flush=True, end='')
file_list_path = f'{EXPORT_DIRECTORY}/oa_file_list.csv'
if not os.path.isfile(file_list_path):
    if file := open(file_list_path, 'wb'):
        ftp.retrbinary('RETR oa_file_list.csv', file.write, blocksize=32*1024*1024)
        print('done')
    else:
        print(f'failed (open file {file_list_path})')
        exit(-1)
else:
    print('done (already cached)')

# Load file list as map between PMCID and path
print('Loading open-access file list into memory ... ', flush=True, end='')
path_lookup = {}
for line in open(file_list_path).readlines():
    parts = line.split(',')
    pmc_id = parts[0].split('/')[-1].rstrip('.tar.gz')

    path_lookup[pmc_id] = parts[0]
print(f'done ({len(path_lookup)} articles)')

# Download all non-downloaded articles
print(fmt_base:='Fetching full articles (may take a while) ...', flush=True, end='')
oa_pmcids = []
archive_paths = {}
for i, (pm_id, pmc_id) in enumerate(pmc_ids.items()):
    # We adjust the statics for the number of unavailable PMCIDs
    completion = len(oa_pmcids)
    total_count = len(pmc_ids) - len(oa_pmcids) + i

    # We update the text in-place using the carriage return \r
    print(f'\r{fmt_base} {completion}/{total_count}', flush=True, end='')

    path = f'{article_paths[pm_id]}/{pmc_id}.tar.gz'

    # If the file is not available in the open-access list, ignore it
    ftp_path = path_lookup.get(pmc_id)
    if ftp_path is None:
        continue

    # Store all used PMCID
    oa_pmcids.append(pmc_id)
    archive_paths[pm_id] = path

    # Skip already downloaded files
    if os.path.isfile(path):
        continue

    # Retry up to three times in case of FTP failure
    attempts_left = 3
    while 0 < attempts_left:
        attempts_left -= 1

        try:
            # Download with larger block size as recommend by Entrez documentation
            if file := open(path, 'wb'):
                ftp.retrbinary(f'RETR {ftp_path}', file.write, blocksize=32*1024*1024)
            else:
                print(f'Failed to open file {path}')
                exit(-1)
        except:
            print(f'\r{fmt_base} {completion}/{total_count} ({attempts_left} attempts left)',
                  flush=True, end='')

            # Setup a new FTP connection to NCBI PMC database
            ftp = ftplib.FTP('ftp.ncbi.nlm.nih.gov')
            ftp.login()
            ftp.cwd('/pub/pmc/')

            continue

# Update the text in-place using the carriage return \r
print(f'\r{fmt_base} done ({len(oa_pmcids)} articles)     ')

# Terminate FTP connection
ftp.quit()

# Extract all archives
print(fmt_base:='Extracting archives ...', flush=True, end='')
failed = []
for i, (pm_id, path) in enumerate(archive_paths.items()):
    # We have to adjust for skipped archives
    print(f'\r{fmt_base} {i}/{len(archive_paths)}', flush=True, end='')

    # Extract the archive, on fail report to user. The usual reason for a fail
    # would be an incomplete archive caused by an interrupted download,
    # therefore remove the archive and later prompt for rerun
    try:
        tarfile.open(path).extractall(article_paths[pm_id])
    except:

        failed.append(pm_id)
        os.remove(path)
        continue

    # Remove the extract directory
    extracted_path = path.removesuffix('.tar.gz')
    for filename in os.listdir(extracted_path):
        shutil.copy(os.path.join(extracted_path, filename), article_paths[pm_id])
    shutil.rmtree(extracted_path)

if 0 == len(failed):
    # We use whitespace padding to make sure we override the previous text
    print(f'\r{fmt_base} done{" " * 10}')
else:
    print(f'\r{fmt_base} failed ({len(failed)} archives failed, rerun the program)')
    exit(-1)

# Download all available XML articles in batches
print(fmt_base:='Downloading PubMed Central open-access articles in XML format ...', flush=True, end='')
for i in range(0, len(oa_pmcids), BATCH_SIZE):
    xml_data = Entrez.efetch(db='pmc', id=oa_pmcids[i:i+BATCH_SIZE],
                             retmax=BATCH_SIZE, rettype='full', retmode='xml').read().decode()

    # Parse the response XML into ElementTree
    xml_root = ET.fromstring(xml_data)

    # Export each article to article.xml
    for article in xml_root.findall('article'):
        pm_id = article.find(".//article-id[@pub-id-type='pmid']").text

        # Write article to file
        path = f'{article_paths[pm_id]}/article.xml'
        if file := open(path, 'wb+'):
            file.write(ET.tostring(article))
        else:
            print(f'\r{fmt_base} failed (open file {path})')
            exit(-1)

    # Output statistics
    completion = min(i+BATCH_SIZE, len(oa_pmcids))
    total_count = len(oa_pmcids)
    print(f'\r{fmt_base} {completion}/{total_count}',
          flush=True, end='')
print(f'\r{fmt_base} done{" " * 10}')

# Output the result to EXPORT_DIRECTORY/results.json
if file := open(file_path:=f'{EXPORT_DIRECTORY}/results.json', 'w'):
    data = { 'articles': article_paths, 'pmc_ids': pmc_ids }

    # Export the data
    json.dump(data, file)
else:
    print(f'Failed to open file {file_path}')
    exit(-1)
